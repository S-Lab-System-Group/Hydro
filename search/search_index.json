{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Surrogate-based Hyperparameter Tuning System <p>Hydro is a system that automatically applies the hyperparameter transfer theory together with multiple system techniques to jointly improve the tuning efficiency.</p> <p>\ud83d\ude80 Efficient Tuning. Hydro scales down the model size and fuses multiple trials can significantly improve training throughput and hardware efficiency.</p> <p>\u2728 Automatic Pipeline. Hydro streamlines the surrogate model generation process and seamlessly integrates with Ray, offering a user-friendly solution for tuning.</p> <p>\ud83c\udf89 Quality Maintenance. Hydro typically can well maintain the tuned model quality even using a scaled and fused model for tuning.</p> <p>Getting Started</p> <ul> <li> <p>Tutorial: Walkthrough of a Hydro Tuning Example</p> </li> <li> <p>More Examples:</p> <ul> <li><code>vision</code>: Image Classification Example</li> <li><code>language</code>: Language Modeling Example</li> </ul> </li> </ul> Need More Support? <p>If you have any question or suggestion on improving Hydro, please let us know.</p>"},{"location":"api/TuneConfig/","title":"TuneConfig","text":""},{"location":"api/TuneConfig/#hydro-args","title":"Hydro args:","text":"<ul> <li> <p><code>scaling_num</code>: The number of model width scaling. Hydro supports switch different tuning modes <code>Hydro (Scaling+Fusion) | Hydro (Fusion Only) | Ray (Classic HPO)</code> via setting this value. Specifically,</p> <p>0 = Using Ray Tune (disable both scaling and fusion),</p> <p>1 = Using Hydro (fusion only, disable scaling),</p> <p>Any integer &gt; 1 (preferably powers of 2) enables both scaling and fusion.</p> <p>Default value is 8.</p> </li> <li> <p><code>fusion_limit</code>: User defined maximum model fusion number. Only work when <code>scaling_num</code> &gt; 0. </p> <p>0 = Disabling fusion (scaling only).</p> <p>1 = Similar with disabling fusion, but will still replace original model with Hydro modules.</p> <p>If set to None, Hydro will automatically profile and determine the actual fusion number according to GPU memory capacity.</p> <p>If set to a positive integer, Hydro will use this value as the fusion limit.</p> <p>If set to a dict, Hydro will use this dict as the fusion for different batch size.</p> <p>Default is None.</p> </li> </ul> <p>If you want to tune <code>batch_size</code> hyperparameter</p> <p>Please do name it as <code>batch_size</code>, other names (like bs, bsz, etc.) will not be recognized by Hydro.</p> <ul> <li> <p><code>eager_transfer</code>: The ratio of maximum trials (<code>num_samples</code>) to start a target model trial. Must in (0, 1].</p> <p>1 = Disabling eager transfer.</p> <p>Default value is 0.5.</p> </li> <li> <p><code>trial_compile</code>: Whether to enable torch.compile() to further accelerate model             training throughput. If enabled, Hydro does not support model checkpointing             and multi-fidelity tuning algorithms. Default is False.</p> </li> </ul>"},{"location":"api/TuneConfig/#source-code","title":"Source Code:","text":"<pre><code>@dataclass\nclass TuneConfig:\n\"\"\"Tune specific configs.\n    Args:\n        ======================================================================\n        Hydro args:\n        scaling_num: The number of model width scaling. Hydro supports switch\n            different tuning modes ``Hydro (Scaling+Fusion) | Hydro (Fusion Only)\n            | Ray (Classic HPO)`` via setting this value. Specifically,\n            0 = Using Ray Tune (disable both scaling and fusion),\n            1 = Using Hydro (fusion only, disable scaling),\n            Any integer &gt; 1 (preferably powers of 2) enables both scaling and fusion\n            Default value is 8.\n        fusion_limit: User defined maximum model fusion number. Only work when\n            `scaling_num` &gt; 0. Default is None.\n            0 = Disabling fusion (scaling only).\n            1 = Similar with disabling fusion, but will still replace original model\n            with Hydro modules.\n            If set to None, Hydro will automatically profile and determine the actual\n            fusion number according to GPU memory capacity.\n            If set to a positive integer, Hydro will use this value as the fusion limit.\n            If set to a dict, Hydro will use this dict as the fusion for different batch size.\n        eager_transfer: The ratio of maximum trials (`num_samples`) to start a\n            target model trial. Must in (0, 1].\n            1 = Disabling eager transfer. Default value is 0.5.\n        trial_compile: Whether to enable torch.compile() to further accelerate model\n            training throughput. If enabled, Hydro does not support model checkpointing\n            and multi-fidelity tuning algorithms. Default is False.\n        ======================================================================\n        Ray args:\n        metric: Metric to optimize. This metric should be reported\n            with `tune.report()`. If set, will be passed to the search\n            algorithm and scheduler.\n        mode: Must be one of [min, max]. Determines whether objective is\n            minimizing or maximizing the metric attribute. If set, will be\n            passed to the search algorithm and scheduler.\n        search_alg: Search algorithm for optimization. Default to\n            random search.\n        scheduler: Scheduler for executing the experiment.\n            Choose among FIFO (default), MedianStopping,\n            AsyncHyperBand, HyperBand and PopulationBasedTraining. Refer to\n            ray.tune.schedulers for more options.\n        num_samples: Number of times to sample from the\n            hyperparameter space. Defaults to 1. If `grid_search` is\n            provided as an argument, the grid will be repeated\n            `num_samples` of times. If this is -1, (virtually) infinite\n            samples are generated until a stopping condition is met.\n        max_concurrent_trials: Maximum number of trials to run\n            concurrently. Must be non-negative. If None or 0, no limit will\n            be applied. This is achieved by wrapping the ``search_alg`` in\n            a :class:`ConcurrencyLimiter`, and thus setting this argument\n            will raise an exception if the ``search_alg`` is already a\n            :class:`ConcurrencyLimiter`. Defaults to None.\n        time_budget_s: Global time budget in\n            seconds after which all trials are stopped. Can also be a\n            ``datetime.timedelta`` object.\n        reuse_actors: Whether to reuse actors between different trials\n            when possible. This can drastically speed up experiments that start\n            and stop actors often (e.g., PBT in time-multiplexing mode). This\n            requires trials to have the same resource requirements.\n            Defaults to ``True`` for function trainables (including most\n            Ray AIR trainers) and ``False`` for class and registered trainables\n            (e.g. RLlib).\n        trial_name_creator: Optional function that takes in a Trial and returns\n            its name (i.e. its string representation). Be sure to include some unique\n            identifier (such as `Trial.trial_id`) in each trial's name.\n            NOTE: This API is in alpha and subject to change.\n        trial_dirname_creator: Optional function that takes in a trial and\n            generates its trial directory name as a string. Be sure to include some\n            unique identifier (such as `Trial.trial_id`) is used in each trial's\n            directory name. Otherwise, trials could overwrite artifacts and checkpoints\n            of other trials. The return value cannot be a path.\n            NOTE: This API is in alpha and subject to change.\n        chdir_to_trial_dir: Whether to change the working directory of each worker\n            to its corresponding trial directory. Defaults to `True` to prevent\n            contention between workers saving trial-level outputs.\n            If set to `False`, files are accessible with paths relative to the\n            original working directory. However, all workers on the same node now\n            share the same working directory, so be sure to use\n            `session.get_trial_dir()` as the path to save any outputs.\n    \"\"\"\n# Currently this is not at feature parity with `tune.run`, nor should it be.\n# The goal is to reach a fine balance between API flexibility and conciseness.\n# We should carefully introduce arguments here instead of just dumping everything.\nmode: Optional[str] = None\nmetric: Optional[str] = None\nsearch_alg: Optional[Union[Searcher, SearchAlgorithm]] = None\nscheduler: Optional[TrialScheduler] = None\nnum_samples: int = 1\nmax_concurrent_trials: Optional[int] = None\ntime_budget_s: Optional[Union[int, float, datetime.timedelta]] = None\nreuse_actors: Optional[bool] = None\ntrial_name_creator: Optional[Callable[[Trial], str]] = None\ntrial_dirname_creator: Optional[Callable[[Trial], str]] = None\nchdir_to_trial_dir: bool = True\nscaling_num: int = 8\nfusion_limit: Optional[Union[int, Dict]] = None\neager_transfer: float = 0.5\ntrial_compile: bool = False\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>We provide three ways to install Hydro, including <code>pip</code>, <code>docker</code> and <code>git</code>. </p> <p>Tip</p> <p>We recommend create a new conda environment to install Hydro. <pre><code>conda create -n hydro python=3.9\n</code></pre></p>"},{"location":"getting_started/installation/#with-pip","title":"with pip recommended","text":"<p>Hydro is published as a Python package and can be installed with <code>pip</code> directly.</p> pip <pre><code>pip install hydro-tune\n</code></pre>"},{"location":"getting_started/installation/#with-docker","title":"with docker","text":"<p>The official Docker image of Hydro is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. </p> docker <pre><code>docker pull tonyhao96/hydro\n</code></pre> <p>Here is an example command of running Hydro in a docker container: <pre><code>docker run --name hydro --shm-size=16g --gpus all -it tonyhao96/hydro\n</code></pre></p> <p>The source code and example are inside <code>/workspace/Hydro</code>.</p>"},{"location":"getting_started/installation/#with-git","title":"with git","text":"<p>If you want to use the very latest version, Hydro can be directly cloned from GitHub.</p> git <pre><code>git clone https://github.com/S-Lab-System-Group/Hydro.git\n</code></pre> <p>Then you can install with <code>pip</code> in the root directory of Hydro.</p> <pre><code>pip install -e Hydro\n</code></pre>"},{"location":"getting_started/quick_start/","title":"Getting started","text":"<p>In this walkthrough, we will present how to tune an image classifier (e.g., ResNet-18 model) on CIFAR-10 dataset using Hydro.</p> <p>Tip</p> <p>Please refer to Ray Docs for more information if you are not familiar with Ray Tune.</p>"},{"location":"getting_started/quick_start/#installation","title":"Installation","text":"<p>To run this example, we need to install Hydro package beforehand. Further installation instructions can be found in here.</p> pip <pre><code>pip install hydro-tune\n</code></pre>"},{"location":"getting_started/quick_start/#import-libraries","title":"Import Libraries","text":"<p>Let's begin by importing the necessary modules:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport ray\nfrom ray import tune\nfrom ray.air.config import RunConfig, ScalingConfig\nfrom hydro import HydroTuner, HydroTrainer, session\nfrom hydro.tune.tune_config import TuneConfig\nimport hydro.train as ht\nfrom filelock import FileLock\nfrom pathlib import Path\n</code></pre>"},{"location":"getting_started/quick_start/#setup-the-search-space","title":"Setup the Search Space","text":"<p>We need to define search space with Ray Tune API. Here is an example: <pre><code>SEARCH_SPACE = {\n\"lr\": tune.qloguniform(1e-4, 1, 1e-4),\n\"momentum\": tune.quniform(0.5, 0.999, 0.001),\n\"batch_size\": tune.choice([128, 256, 512]),\n}\n</code></pre> The <code>tune.qloguniform(lower, upper, q)</code> function samples in different orders of magnitude and quantizes the value to an integer increment of <code>q</code>. For more search space functions, please refer to Ray Tune Search Space API.</p>"},{"location":"getting_started/quick_start/#load-dataset","title":"Load Dataset","text":"<p>We first load the CIFAR10 dataset and use a <code>FileLock</code> to prevent multiple processes from downloading the same data.</p> <pre><code>def get_dataset():\nnormalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\nwith FileLock(Path(\"~/data/data.lock\").expanduser()):\ntrain_dataset = datasets.CIFAR10(\nroot=\"~/data\",\ntrain=True,\ndownload=True,\ntransform=transforms.Compose(\n[transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]\n),\n)\nval_dataset = datasets.CIFAR10(\nroot=\"~/data\", train=False, download=False, transform=transforms.Compose([transforms.ToTensor(), normalize])\n)\nreturn train_dataset, val_dataset\n</code></pre>"},{"location":"getting_started/quick_start/#train-validation-function","title":"Train &amp; Validation Function","text":"<p>To support inter-trial fusion feature, we add <code>fusion_num</code> as an argument to the train and validation functions. Besides, we need to incorporate some code to resize specific tensors as highlighted below.</p> <pre><code>def train_epoch(dataloader, model, optimizer, fusion_num):\nmodel.train()\nfor _, (X, y) in enumerate(dataloader):\nif fusion_num &gt; 0:\nX = X.unsqueeze(1).expand(-1, fusion_num, -1, -1, -1).contiguous()\ny = y.repeat(fusion_num)\npred = model(X)\nif fusion_num &gt; 0:\nlosses = (\nnn.CrossEntropyLoss(reduction=\"none\")(pred.contiguous().view(y.size(0), -1), y).view(fusion_num, -1).mean(dim=1)\n)\nloss = losses.mean()\nelse:\nloss = nn.CrossEntropyLoss(pred, y)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\ndef validate_epoch(dataloader, model, fusion_num):\nsize = len(dataloader.dataset) // session.get_world_size()\nnum_batches = len(dataloader)\nmodel.eval()\ntest_loss, correct = 0, 0\nwith torch.no_grad():\nfor X, y in dataloader:\nif fusion_num &gt; 0:\nX = X.unsqueeze(1).expand(-1, fusion_num, -1, -1, -1).contiguous()\ny = y.repeat(fusion_num)\npred = model(X)\nif fusion_num &gt; 0:\ntest_loss += (\nnn.CrossEntropyLoss(reduction=\"none\")(pred.contiguous().view(y.size(0), -1), y)\n.view(fusion_num, -1)\n.mean(dim=1)\n)\npred = pred.argmax(dim=2, keepdim=True)\ncorrect += pred.eq(y.view_as(pred)).view(fusion_num, -1).sum(dim=1).float()\nelse:\ntest_loss += nn.CrossEntropyLoss(pred, y).item()\ncorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\ntest_loss /= num_batches\ncorrect /= size\nif fusion_num &gt; 0:\nreturn {\"loss\": test_loss.cpu().detach().tolist(), \"val_acc\": correct.cpu().detach().tolist()}\nelse:\nreturn {\"loss\": test_loss, \"val_acc\": correct}\n</code></pre>"},{"location":"getting_started/quick_start/#wrap-model-optimizer-and-dataloader","title":"Wrap Model, Optimizer and DataLoader","text":"<p>We need to wrap model, optimizer and dataLoader with <code>hydro.train</code> api.</p> <pre><code>def train_func(config):\nht.accelerate(config)\nfusion_num = config.get(\"FUSION_N\", 0)\nmodel = torchvision.models.__dict__[\"resnet18\"]()\nmodel = ht.prepare_model(model)\noptimizer = ht.prepare_optimizer(\ntorch.optim.SGD,\nmodel.parameters(),\nlr=config.get(\"lr\", 0.01),\nmomentum=config.get(\"momentum\", 0.9),\nweight_decay=config.get(\"weight_decay\", 0.001),\n)\nworker_batch_size = config[\"batch_size\"] // session.get_world_size()\ntrain_set, val_set = get_dataset()\ntrain_loader = DataLoader(train_set, batch_size=worker_batch_size, pin_memory=True, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=worker_batch_size, pin_memory=True)\ntrain_loader = ht.prepare_data_loader(train_loader)\nval_loader = ht.prepare_data_loader(val_loader)\nfor _ in range(10000): # Not determine the actual epoch number\ntrain_epoch(train_loader, model, optimizer, fusion_num)\nresult = validate_epoch(val_loader, model, fusion_num)\nsession.report(result)\n</code></pre>"},{"location":"getting_started/quick_start/#configure-tuner","title":"Configure Tuner","text":"<p><code>HydroTuner</code> is the key interface of configuring hyperparameter tuning job. Users can specify maximum number of trials <code>num_samples</code>, maximum epochs <code>stop</code>, model scaling ratio <code>scaling_num</code> and inter-trial fusion limit <code>fusion_limit</code>.</p> <pre><code>if __name__ == \"__main__\":\nray.init(address=None)\ntrainer = HydroTrainer(\ntrain_func,\ntrain_loop_config=SEARCH_SPACE,\nscaling_config=ScalingConfig(\nnum_workers=1,\nuse_gpu=True,\nresources_per_worker={\"CPU\": 2, \"GPU\": 1},\n),\n)\ntuner = HydroTuner(\ntrainer,\nparam_space={\"train_loop_config\": SEARCH_SPACE},\ntune_config=TuneConfig(\nnum_samples=50,\nmetric=\"val_acc\",\nmode=\"max\",\nscaling_num=8,  # Hydro args\nfusion_limit=10,  # Hydro args\n),\nrun_config=RunConfig(\nstop={\"training_iteration\": 50},\n),\n)\nresults = tuner.fit()\n</code></pre>"},{"location":"getting_started/quick_start/#example-output","title":"Example Output","text":"<p>After tuning the models, we will find the best performing one and load the trained network from the checkpoint file. We then obtain the test set accuracy and report everything by printing.</p> <p>If you run the code, an example output could look like this:</p> <pre><code>== Status ==\nCurrent time: 2023-04-25 08:20:42 (running for 00:23:42.54)\nMemory usage on this node: 25.5/251.5 GiB Using FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0/4 GPUs, 0.0/157.96 GiB heap, 0.0/71.69 GiB objects (0.0/1.0 accelerator_type:G)\nCurrent best trial: b38f6_T0001(target trial) with val_acc=0.9162 and parameters={'lr': 0.1102, 'momentum': 0.584, 'batch_size': 128, 'gamma': 0.14, 'dataset': 'cifar10', 'seed': 10, 'FUSION_N': 0, 'SCALING_N': 0}\nResult logdir: ~/ray_results\nNumber of trials: 8/50 (8 TERMINATED)\n+--------------------------+------------+----------------------+----------+------+----------------------+----------------------+----------------------+--------+------------------+--------------+---------------------+-----------------------+\n| Trial name               | status     | loc                  | hydro    |   bs | gamma                | lr                   | momentum             |   iter |   total time (s) |   _timestamp |   _time_this_iter_s |   _training_iteration |\n|--------------------------+------------+----------------------+----------+------+----------------------+----------------------+----------------------+--------+------------------+--------------+---------------------+-----------------------|\n| HydroTrainer_b38f6_T0001 | TERMINATED | 10.100.79.96:3657182 | Target   |  128 | 0.14                 | 0.1102               | 0.584                |     50 |          384.496 |   1682382041 |             7.71278 |                    50 |\n| HydroTrainer_b38f6_T0000 | TERMINATED | 10.100.79.96:3479472 | Target   |  512 | 0.05                 | 0.1827               | 0.846                |     50 |          279.453 |   1682381326 |             5.47435 |                    50 |\n| HydroTrainer_b38f6_F0000 | TERMINATED | 10.100.79.96:3223763 | F=9, S=8 |  256 | [0.74, 0.38, 0._df80 | [0.0204, 0.4689_6d40 | [0.584, 0.857, _dec0 |     50 |          427.166 |   1682381050 |             8.64703 |                    50 |\n| HydroTrainer_b38f6_F0001 | TERMINATED | 10.100.79.96:3223967 | F=8, S=8 |  256 | [0.32, 0.31, 0._fb80 | [0.013600000000_23c0 | [0.507, 0.69400_6500 |     50 |          415.149 |   1682381041 |             9.09435 |                    50 |\n| HydroTrainer_b38f6_F0002 | TERMINATED | 10.100.79.96:3223968 | F=9, S=8 |  512 | [0.46, 0.28, 0._2b00 | [0.004500000000_f080 | [0.615, 0.659, _2180 |     50 |          382.011 |   1682381008 |             7.61978 |                    50 |\n| HydroTrainer_b38f6_F0003 | TERMINATED | 10.100.79.96:3223969 | F=8, S=8 |  512 | [0.04, 0.4, 0.0_eb40 | [0.0011, 0.1303_eec0 | [0.65, 0.791, 0_fe00 |     50 |          357.47  |   1682380984 |             6.90358 |                    50 |\n| HydroTrainer_b38f6_F0004 | TERMINATED | 10.100.79.96:3451196 | F=8, S=8 |  128 | [0.14, 0.54, 0._8280 | [0.1102, 0.2675_a200 | [0.584, 0.675, _8400 |     50 |          773.026 |   1682381761 |            15.4453  |                    50 |\n| HydroTrainer_b38f6_F0005 | TERMINATED | 10.100.79.96:3464377 | F=8, S=8 |  128 | [0.42, 0.54, 0._a140 | [0.307100000000_e880 | [0.981, 0.81800_8f40 |     50 |          737.031 |   1682381749 |            14.6641  |                    50 |\n+--------------------------+------------+----------------------+----------+------+----------------------+----------------------+----------------------+--------+------------------+--------------+---------------------+-----------------------+\n</code></pre>"},{"location":"getting_started/quick_start/#see-more-pytorch-examples","title":"See More PyTorch Examples","text":""},{"location":"getting_started/quick_start/#vision-image-classification-example","title":"<code>vision</code>: Image Classification Example","text":"<ul> <li> <p>run_hydro.py</p> <p>Tuning ResNet-18 on CIFAR-10 dataset using Hydro.</p> </li> <li> <p>run_ray.py</p> <p>The original Ray Tune script for reference.</p> </li> </ul>"},{"location":"getting_started/quick_start/#language-language-modeling-example","title":"<code>language</code>: Language Modeling Example","text":"<ul> <li> <p>run_hydro_lm.py</p> <p>Tuning HuggingFace GPT-2 on WikiText dataset using Hydro. To be compatible with most machines, we set <code>n_layer=2</code> by default.</p> </li> <li> <p>run_ray_lm.py</p> <p>The original Ray Tune script for reference.</p> </li> </ul>"}]}